{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Neural Networks\n",
    "##### Structure of a Neural Network:\n",
    "- **Input Layer**: Takes input features (e.g., pixels in an image).\n",
    "- **Hidden Layers**: Learn complex patterns from data.\n",
    "- **Output Layer**: Provides the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "  Input Layer    Hidden Layer    Output Layer\n",
    "      O               O               O\n",
    "    /   \\           /   \\           /\n",
    "  O       O -----> O     O -----> O  \n",
    "    \\   /           \\   /          \n",
    "      O               O  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Simple Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (Hours Studied, Sleep Hours)\n",
    "X = np.array([[1, 2], [2, 3], [3, 5], [4, 2], [5, 6], [6, 7], [7, 8], [8, 7]])\n",
    "# Output (1 = Pass, 0 = Fail)\n",
    "y = np.array([[0], [0], [1], [0], [1], [1], [1], [1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Activation Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of Sigmoid (for Backpropagation)\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Initialize Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # For reproducibility\n",
    "weights = np.random.rand(2, 1)  # Two input features, one output\n",
    "bias = np.random.rand(1)\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Neural Network (Forward + Backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3564\n",
      "Epoch 1000, Loss: 0.0105\n",
      "Epoch 2000, Loss: 0.0057\n",
      "Epoch 3000, Loss: 0.0039\n",
      "Epoch 4000, Loss: 0.0029\n",
      "Epoch 5000, Loss: 0.0023\n",
      "Epoch 6000, Loss: 0.0019\n",
      "Epoch 7000, Loss: 0.0017\n",
      "Epoch 8000, Loss: 0.0015\n",
      "Epoch 9000, Loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000  # Number of iterations\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward Propagation\n",
    "    Z = np.dot(X, weights) + bias\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    # Compute Error\n",
    "    error = y - A\n",
    "\n",
    "    # Backpropagation\n",
    "    dZ = error * sigmoid_derivative(A)  # Gradient of Loss w.r.t Z\n",
    "    weights += np.dot(X.T, dZ) * learning_rate  # Update Weights\n",
    "    bias += np.sum(dZ) * learning_rate  # Update Bias\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(error ** 2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (Probability of Passing): [[0.8900576]]\n"
     ]
    }
   ],
   "source": [
    "# Predict for new data\n",
    "new_student = np.array([[4, 5]])  # 4 study hours, 5 sleep hours\n",
    "prediction = sigmoid(np.dot(new_student, weights) + bias)\n",
    "print(\"Prediction (Probability of Passing):\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Deep Learning Model using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (Hours Studied, Sleep Hours)\n",
    "X = np.array([[1, 2], [2, 3], [3, 5], [4, 2], [5, 6], [6, 7], [7, 8], [8, 7]], dtype=np.float32)\n",
    "# Output (1 = Pass, 0 = Fail)\n",
    "y = np.array([[0], [0], [1], [0], [1], [1], [1], [1]], dtype=np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 4)  # 2 input features → 4 neurons in the hidden layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.output = nn.Linear(4, 1)  # 4 hidden neurons → 1 output neuron\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss (for binary classification)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer with learning rate 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7094\n",
      "Epoch 100, Loss: 0.2709\n",
      "Epoch 200, Loss: 0.0643\n",
      "Epoch 300, Loss: 0.0275\n",
      "Epoch 400, Loss: 0.0154\n",
      "Epoch 500, Loss: 0.0098\n",
      "Epoch 600, Loss: 0.0067\n",
      "Epoch 700, Loss: 0.0049\n",
      "Epoch 800, Loss: 0.0037\n",
      "Epoch 900, Loss: 0.0029\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000  # Number of training iterations\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (Probability of Passing): [[0.9874164]]\n"
     ]
    }
   ],
   "source": [
    "# Test data (Predict if a student who studied 4 hours and slept 5 hours will pass)\n",
    "new_student = torch.tensor([[4, 5]], dtype=torch.float32)\n",
    "prediction = model(new_student).detach().numpy()\n",
    "print(\"Prediction (Probability of Passing):\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Scaled Dot-Product Attention in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Define Input Sentences\n",
    "Each word will be represented as a vector (embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query (Q):\n",
      " [[0.37454012 0.95071431 0.73199394]\n",
      " [0.59865848 0.15601864 0.15599452]\n",
      " [0.05808361 0.86617615 0.60111501]\n",
      " [0.70807258 0.02058449 0.96990985]]\n",
      "Key (K):\n",
      " [[0.83244264 0.21233911 0.18182497]\n",
      " [0.18340451 0.30424224 0.52475643]\n",
      " [0.43194502 0.29122914 0.61185289]\n",
      " [0.13949386 0.29214465 0.36636184]]\n",
      "Value (V):\n",
      " [[0.45606998 0.78517596 0.19967378]\n",
      " [0.51423444 0.59241457 0.04645041]\n",
      " [0.60754485 0.17052412 0.06505159]\n",
      " [0.94888554 0.96563203 0.80839735]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example sentence with 4 words (each word is a 3D vector)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "words = 4  # Number of words\n",
    "embedding_dim = 3  # Word vector size\n",
    "\n",
    "# Random word embeddings\n",
    "Q = np.random.rand(words, embedding_dim)\n",
    "K = np.random.rand(words, embedding_dim)\n",
    "V = np.random.rand(words, embedding_dim)\n",
    "\n",
    "print(\"Query (Q):\\n\", Q)\n",
    "print(\"Key (K):\\n\", K)\n",
    "print(\"Value (V):\\n\", V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Compute Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Attention Scores:\n",
      " [[0.64675177 0.74205833 0.88652906 0.59816679]\n",
      " [0.55984141 0.23912325 0.39947042 0.18623963]\n",
      " [0.34157207 0.58961914 0.64513862 0.48137664]\n",
      " [0.77015453 0.64509281 0.90528538 0.46012339]]\n"
     ]
    }
   ],
   "source": [
    "# Compute dot product of Query and Key (QK^T)\n",
    "attention_scores = np.dot(Q, K.T)\n",
    "print(\"\\nRaw Attention Scores:\\n\", attention_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Apply Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled Scores:\n",
      " [[0.37340231 0.42842758 0.51183779 0.34535176]\n",
      " [0.32322459 0.13805788 0.23063436 0.1075255 ]\n",
      " [0.19720672 0.34041677 0.37247096 0.27792293]\n",
      " [0.44464892 0.37244451 0.52266676 0.26565236]]\n"
     ]
    }
   ],
   "source": [
    "d_k = embedding_dim  # Scaling factor\n",
    "scaled_scores = attention_scores / np.sqrt(d_k)\n",
    "print(\"\\nScaled Scores:\\n\", scaled_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Softmax to Normalize Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention Weights:\n",
      " [[0.23938437 0.2529257  0.27492711 0.23276281]\n",
      " [0.28180838 0.23417311 0.25688721 0.2271313 ]\n",
      " [0.2257542  0.26051406 0.26899991 0.24473182]\n",
      " [0.25990286 0.24179821 0.28099188 0.21730706]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)\n",
    "print(\"\\nAttention Weights:\\n\", attention_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiply by Value Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Output (Self-Attention result):\n",
      " [[0.62713495 0.60944066 0.26559657]\n",
      " [0.62053633 0.62312745 0.26747044]\n",
      " [0.63257702 0.61378096 0.27251761]\n",
      " [0.61978955 0.60506881 0.25707683]]\n"
     ]
    }
   ],
   "source": [
    "# Multiply attention weights by the Value matrix\n",
    "output = np.dot(attention_weights, V)\n",
    "print(\"\\nFinal Output (Self-Attention result):\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Basic Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Positional Encoding\n",
    "Since Transformers don’t have recurrence, we use a mathematical function to encode word positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Multi-Head Attention\n",
    "Instead of computing attention once, Multi-Head Attention allows the model to focus on different words at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0  # Ensure dimensions match\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size = Q.shape[0]\n",
    "\n",
    "        # Linear transformations\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Concatenate heads\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        return self.W_o(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Transformer Block\n",
    "**Each Transformer Block consists of:**\n",
    "1. Multi-Head Attention\n",
    "2. Feed-Forward Neural Network\n",
    "3. Layer Normalization\n",
    "4. Residual Connections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_output)  # Residual Connection + Normalization\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)  # Residual Connection + Normalization\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
